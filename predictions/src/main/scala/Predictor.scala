/**
  * Usage:
  *
  * 1) Build from the predictions folder with `sbt assembly`
  * 2) Run (on the server with MapR CLI set up) with
  *   java -jar iot-predictions-assembly-1.0.jar [threshold=0.003|failureRate=0.005|features=test1,test2,test3|timer=100|predictionCacheSize=1000]
  */

import java.util.Properties

import hex.genmodel.GenModel
import io.confluent.kafka.serializers.KafkaJsonSerializer
import org.apache.kafka.clients.consumer.{ConsumerRecord, ConsumerRecords, OffsetAndMetadata}
import org.apache.kafka.clients.producer.{KafkaProducer, ProducerRecord}
import org.apache.kafka.common.TopicPartition

import scala.collection.mutable
import scala.io.Source

object Predictor {

  // H2O generated POJO model name
  val modelClassName = "iot_dl"

  // Queues to read our data from/write our predictions to
  val sensorTopic = "/streams/sensor:sensor1"
  val predictionTopic = "/streams/sensor:sensor-state-test"
  // Kafka producer
  val producer: KafkaProducer[String, Int] = makeProducer()

  // Current state of the machine
  var state = 0

  // Prediction configurations
  // Prediction window size, has to be the same as the window size used to model training
  var window: Int = 200
  // How long are persisting predictions
  var predictionCacheSize: Int = window*5
  // How many failure predictions do we consider as an actual failure.
  var failureRate: Double = 5.0/predictionCacheSize.toDouble

  // Feature names
  var headers = Array("LinAccX..g.","LinAccY..g.","LinAccZ..g.")
  // Number of features per observation
  var features: Int = headers.length
  // How often we send status updates to the system
  var timer: Double = window * 10

  var threshold: Double = {
    val props = new Properties()
    props.load(Source.fromURL(getClass.getResource("/dl.properties")).bufferedReader())
    val t = props.get("threshold").toString.toDouble
    println(s"Setting threshold to $t")
    t
  }

  // Create a Kafka producer
  private def makeProducer() = {
    val props = new Properties()

    props.put("key.serializer", classOf[KafkaJsonSerializer[String]].getCanonicalName)
    props.put("value.serializer", classOf[KafkaJsonSerializer[String]].getCanonicalName)

    new KafkaProducer[String, Int](props)
  }

  def main(args: Array[String]): Unit = {
    // Parse command line config
    for(arg <- args) {
      val kv = arg.split("=")
      if(kv(0) == "threshold") {
        print(s"Setting threshold to ${kv(1).toDouble} from the command line.")
        threshold = kv(1).toDouble
      } else if(kv(0) == "features") {
        print(s"Setting headers to [${kv(1)}]")
        headers = kv(1).split(",")
        features = headers.length
      } else if(kv(0) == "failureRate") {
        print(s"Setting failureRate to [${kv(1)}]")
        failureRate = kv(1).toDouble
      } else if(kv(0) == "timer") {
        print(s"Setting timer to [${kv(1)}]")
        timer = kv(1).toDouble
      } else if(kv(0) == "predictionCacheSize") {
        print(s"Setting predictionCacheSize to [${kv(1)}]")
        predictionCacheSize = kv(1).toInt
        failureRate = 1.0/(5.0*predictionCacheSize.toDouble)
      }
    }

    // Sender sending data to the Kafka queue
    new Thread() {
      override def run(): Unit = {
        while(true) {
          Thread.sleep(timer.toLong)
          pushPrediction(state)
        }
      }
    }.start()

    // Kafka consumer reading sensor data from the queue
    val consumer = new MapRStreamsConsumerFacade(sensorTopic)
    try {
      consumer.prepareSetup()
      println("Prepared")
      consumer.open()
      println("Opened")
      poll(consumer)
    } finally {
      producer.close()
      consumer.close()
    }
  }

  private def pushPrediction(label: Int) = {
    println(s"Pushing prediction $label")

    producer.send(
      new ProducerRecord[String, Int](
        predictionTopic,
        "state",
        label
      )
    )
  }

  import scala.collection.JavaConversions._

  def poll(consumer: MapRStreamsConsumerFacade): Unit = {
    val fullWindowFeatures = features * window

    // Data used for predictions
    val inputRB: RingBuffer[Double] = new RingBuffer(fullWindowFeatures)

    // Model generated by H2O
    val rawModel: GenModel = Class.forName(modelClassName).newInstance().asInstanceOf[GenModel]

    // Previous predictions
    val rb: RingBuffer[Int] = new RingBuffer(predictionCacheSize)
    while(true) {
      val commitMap = new mutable.LinkedHashMap[TopicPartition, OffsetAndMetadata]()

      val records: ConsumerRecords[String, String] = consumer.poll()
      println("Polled " + records.count())

      for(record: ConsumerRecord[String, String] <- records) {
        val split = record.value().replaceAll("\"", "").split(",")
        if(split.length >= features) {
          val input = split.takeRight(features).map(_.toDouble)
          for(i <- input) {
            inputRB.+=(i)
          }

          // If we have enough readings we can start predicting, we need to wait until we have WINDOW number of readings
          if(inputRB.length == fullWindowFeatures) {
            val preds = Array.fill[Double](fullWindowFeatures){0}
            // Making a prediction
            val pred = rawModel.score0(inputRB.toArray, preds)

            // Calculating the mean squared error of our prediction
            val rmse = inputRB.zip(pred).map { case (i, p) => (p - i) * (p - i) }.sum / (fullWindowFeatures).toDouble

            // If our RMSE if big enough we classify it as a failure 1, otherwise 0
            val label = if (rmse > threshold) 1 else 0

            rb.+=(label)

            // If failure rate % of predictions in our cache are failures - set the state to failed
            if ((rb.sum.toDouble / rb.length.toDouble) >= failureRate) {
              state = 1
            } else {
              state = 0
            }
          }
        }
      }

      if (commitMap.nonEmpty) {
        // Notify the Kafka consumer we got the data
        consumer.commit(commitMap.toMap[TopicPartition, OffsetAndMetadata])
      }
    }
  }

}
